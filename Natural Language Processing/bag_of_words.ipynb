{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bag_of_words.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "Cnvj2xQqLtzW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "quoting = 3 ignora tutte i doppi apici \" nel testo. Evita errori in runtime perchè i metodi potrebbero confonderle con delle stringhe ecc."
      ],
      "metadata": {
        "id": "oOJBbmgATN5h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv('Restaurant_Reviews.tsv', delimiter = '\\t', quoting = 3)"
      ],
      "metadata": {
        "id": "XoZKrAVGSFLb"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il dataset deve essere semplificato e ridotto eliminando parole inutili, verbi in tempi diversi ecc.\n",
        "\n",
        "Le \"stopwords\" sono tutte le congiunzioni, articoli ecc. che non danno significato in piu alla frase.\n",
        "\n",
        "PorterStemmer permette di individuare la parte importante di una parola, come ad esempio la parte al presente del verbo coniugato. (Es: love e loved dovranno occupare la stessa casella nella bag of words)\n",
        "\n",
        "Le stopwords includono anche \"not\", che però non deve essere tolto dalle parole perchè è importante, in quanto cambia completamente il senso della frase."
      ],
      "metadata": {
        "id": "j_S6dLRoUNua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "corpus = []\n",
        "for i in range(0, 1000):\n",
        "  review = re.sub('[^a-zA-Z]', ' ', dataset[\"Review\"][i])\n",
        "  review = review.lower()\n",
        "  review = review.split()\n",
        "  ps = PorterStemmer()\n",
        "  all_stopwords = stopwords.words(\"english\")\n",
        "  all_stopwords.remove(\"not\")\n",
        "  review = [ps.stem(word) for word in review if not word in set(all_stopwords)]\n",
        "  review = \" \".join(review)\n",
        "  corpus.append(review)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUq61Ic_TuJE",
        "outputId": "a967633c-4d85-477e-fc69-f11d162bdd65"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creare il modello bag of words significa creare una matrice sparsa in cui le righe sono le varie frasi, e nelle colonne sono contate le parole che appaiono nella frase. Ogni casella corrisponde ad una parola del dizionario. Il processo si chiama \"tokenization\".\n",
        "\n",
        "E' consigliato inserire il numero massimo di parole contenute nella matrice. Per farlo è meglio far girare il programma una volta, verificare quante parole sono presenti, e poi diminuire il numero massimo in funzione di quante parole \"rare\" o poco rilevanti sono presenti (es: nomi di persona)\n",
        "\n",
        "Ci sono 1566 parole, per cui è fattibile mettere come limite massimo 1500."
      ],
      "metadata": {
        "id": "9Fo7fJk39dZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer(max_features=1500)\n",
        "X = cv.fit_transform(corpus).toarray()\n",
        "y = dataset.iloc[:, -1].values\n",
        "print(len(X[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_c5_wkZR9dvI",
        "outputId": "88a99ac5-a733-475c-8e6f-044083f4ee6d"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)"
      ],
      "metadata": {
        "id": "QcZ_P1AHC0C7"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Per esperienza, l'algoritmo Naive Bayes funziona bene per il NLP"
      ],
      "metadata": {
        "id": "Po0PdzQ_C86r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "classifier = GaussianNB(priors=None, var_smoothing=1e-09)\n",
        "classifier.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jEAArL5C7xj",
        "outputId": "0b64d404-8fdf-40af-b38d-5e16bda470b1"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GaussianNB()"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = classifier.predict(X_test)\n",
        "delta = abs(y_pred-y_test)\n",
        "# print(np.concatenate((y_pred.reshape(len(y_pred),1),y_test.reshape(len(y_test),1), delta.reshape(len(delta),1) ), axis=1))"
      ],
      "metadata": {
        "id": "RIKS-kBZDGC6"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mas6D2nxDKGN",
        "outputId": "0cae6891-b81c-4a80-df76-5544f10480ca"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 67  50]\n",
            " [ 20 113]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.72"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Per fare la previsione di una singola recensione è necessario fare tutto il pre processing effettuato per l'intero dataset, applicato però solo alla singola frase."
      ],
      "metadata": {
        "id": "itK_rLDnEv-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_review = 'I love this restaurant so much'\n",
        "new_review = re.sub('[^a-zA-Z]', ' ', new_review)\n",
        "new_review = new_review.lower()\n",
        "new_review = new_review.split()\n",
        "ps = PorterStemmer()\n",
        "all_stopwords = stopwords.words('english')\n",
        "all_stopwords.remove('not')\n",
        "new_review = [ps.stem(word) for word in new_review if not word in set(all_stopwords)]\n",
        "new_review = ' '.join(new_review)\n",
        "new_corpus = [new_review]\n",
        "new_X_test = cv.transform(new_corpus).toarray()\n",
        "new_y_pred = classifier.predict(new_X_test)\n",
        "print(new_y_pred)\n",
        "\n",
        "new_review = 'I hate this restaurant so much'\n",
        "new_review = re.sub('[^a-zA-Z]', ' ', new_review)\n",
        "new_review = new_review.lower()\n",
        "new_review = new_review.split()\n",
        "ps = PorterStemmer()\n",
        "all_stopwords = stopwords.words('english')\n",
        "all_stopwords.remove('not')\n",
        "new_review = [ps.stem(word) for word in new_review if not word in set(all_stopwords)]\n",
        "new_review = ' '.join(new_review)\n",
        "new_corpus = [new_review]\n",
        "new_X_test = cv.transform(new_corpus).toarray()\n",
        "new_y_pred = classifier.predict(new_X_test)\n",
        "print(new_y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nBlM4NrEuKP",
        "outputId": "23a025ec-055f-4229-c868-fca7c3886a5d"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1]\n",
            "[0]\n"
          ]
        }
      ]
    }
  ]
}